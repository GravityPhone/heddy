The general concepts in the integration plan outlined in the text file are:

1. Handling streaming interactions with the OpenAI API
2. Constructing content for the interaction
3. Initializing an EventHandler with the StreamingManager instance
4. Capturing the response text during the streaming interaction
5. Parsing the response text to check for function calls
6. Executing the function call (e.g., send_text_message) if detected
7. Submitting the tool outputs using the submit_tool_outputs_stream method
8. Continuing the streaming interaction to capture the remaining response text
9. Updating the response text in real-time using the EventHandler
10. Storing the final response text in the response_text attribute of the StreamingManager instance
11. Synthesizing and playing back the final response text

This plan will achieve your goal of continuing to get the rest of the AI interaction because:

1. The EventHandler class is designed to handle various streaming events, such as text creation, text deltas, and tool call deltas. It ensures that the response text is updated in real-time as the interaction progresses.

2. After submitting the tool outputs using the submit_tool_outputs_stream method, the streaming interaction continues to capture any remaining response text. This means that you will receive the complete response from the OpenAI assistant, even after the tool outputs are submitted.

3. The response text is continuously updated and printed for debugging purposes, allowing you to monitor the progress of the interaction.

4. Once the streaming interaction is complete, the final response text is available in the response_text attribute of the StreamingManager instance. You can then parse this response text and send it to be synthesized without requiring any additional event-based programming.

To integrate this plan into your existing codebase with minimal changes, you can:

1. Update the handle_streaming_interaction method in the StreamingManager class to include the logic for submitting tool outputs and continuing the streaming interaction.

2. Modify the EventHandler class to include the necessary methods for handling text creation, text deltas, and tool call deltas. These methods will update the response_text attribute of the StreamingManager instance.

3. Ensure that the synthesize step is triggered after the streaming interaction is complete and the final response text is available.

By following this approach, you can handle the scenario of continuing the AI interaction after submitting tool outputs without significantly altering your existing event-based programming paradigms. The changes will be localized to the StreamingManager and EventHandler classes, minimizing the impact on the rest of your codebase.

Remember to thoroughly test the integration to ensure that it works as expected and doesn't introduce any unintended side effects.

---

The process of handling streaming interactions with the OpenAI API involves several key steps to ensure that user messages are processed correctly, tool outputs are submitted, and responses are synthesized and played back seamlessly. Initially, the streaming interaction is set up by verifying that the `assistant_id` and `thread_id` are properly configured. If these identifiers are not set, appropriate error messages are returned. Once the identifiers are confirmed, the content for the interaction is constructed, and the response text is reset to ensure a clean slate for the new interaction. An `EventHandler` is then initialized with the `StreamingManager` instance to manage the streaming events. The streaming interaction begins by invoking the `openai_client.beta.threads.runs.stream` method, which captures the response text as the interaction progresses. During this process, the response text is continuously updated and printed for debugging purposes. After the initial streaming interaction, the response text is parsed to check for any function calls. If a function call is detected, such as `send_text_message`, the parameters for the function are extracted and the function is executed. Upon successful execution of the function, the tool outputs are prepared and submitted using the `submit_tool_outputs_stream` method. This method ensures that the tool outputs are correctly submitted and the streaming interaction continues to capture any remaining response text. The `EventHandler` class plays a crucial role in this process by handling various streaming events, such as text creation, text deltas, and tool call deltas. It ensures that the response text is updated in real-time and any tool outputs are appended to the response text. Finally, the response text is stored in the `response_text` attribute of the `StreamingManager` instance. Once the streaming interaction is complete, the final response text is available in this attribute. You can then parse this response text and send it to be synthesized and played back to the user, ensuring a seamless and interactive experience. This comprehensive approach leverages the latest capabilities of the OpenAI API to manage streaming interactions effectively, handle tool calls, and provide real-time responses to user queries.

# Step 4: Submit Tool Outputs and Continue Streaming

def handle_streaming_interaction(self, event: ApplicationEvent) -> ApplicationEvent:
    if not self.assistant_id:
        print("Assistant ID is not set.")
        return ApplicationEvent(
            type=ApplicationEventType.ERROR,
            request="Assistant ID is not set.",
            data=None
        )
    if not self.thread_manager.thread_id:
        self.thread_manager.create_thread()

    content = self.construct_content(event)
    print(f"Constructed content: {content}")

    try:
        self.response_text = ""  # Reset the response text at the beginning of the interaction
        self.set_event_handler(EventHandler(self))  # Initialize EventHandler with StreamingManager instance
        with self.openai_client.beta.threads.runs.stream(
            thread_id=self.thread_manager.thread_id,
            assistant_id=self.assistant_id,
            event_handler=self.event_handler,
        ) as stream:
            stream.until_done()
        response_text = self.response_text  # Use the stored response text

        print(f"Response text: {response_text}")

        if not event.data:
            print("Event data is None, initializing to empty dictionary.")
            event.data = {}
        print(f"Event data after initialization: {event.data}")
        if 'required_action' not in event.data:
            event.data['required_action'] = {"submit_tool_outputs": {"tool_calls": []}}
        elif event.data['required_action'] is None:
            raise ValueError("'required_action' is explicitly set to null in event data")

        # Check if the response requires a function call
        if isinstance(response_text, str):
            try:
                response_text = json.loads(response_text)
            except json.JSONDecodeError:
                print(f"Result is not a valid JSON: {response_text}")
                return ApplicationEvent(
                    type=ApplicationEventType.ERROR,
                    request="Invalid response format",
                    data=None
                )

        print(f"Parsed response_text: {response_text}")

        if "function_call" in response_text:
            function_call = response_text["function_call"]
            print(f"Function call detected: {function_call}")
            if function_call["name"] == "send_text_message":
                parameters = function_call["parameters"]
                if isinstance(parameters, str):
                    parameters = json.loads(parameters)
                print(f"Function call parameters: {parameters}")
                zapier_response = send_text_message(parameters)
                if zapier_response == "Success!":
                    run_id = self.thread_manager.run_id
                    tool_outputs = [{
                        "tool_call_id": function_call["id"],
                        "output": zapier_response
                    }]
                    with self.openai_client.beta.threads.runs.submit_tool_outputs_stream(
                        thread_id=self.thread_manager.thread_id,
                        run_id=run_id,
                        tool_outputs=tool_outputs,
                        event_handler=self.event_handler,
                    ) as stream:
                        for text in stream.text_deltas:
                            print(text, end="", flush=True)
                        print()
                    response_text = self.response_text

                    return ApplicationEvent(
                        type=ApplicationEventType.SYNTHESIZE,
                        request=response_text
                    )
                else:
                    return ApplicationEvent(
                        type=ApplicationEventType.ERROR,
                        request=zapier_response
                    )
        else:
            # Handle normal response
            return ApplicationEvent(
                type=ApplicationEventType.SYNTHESIZE,
                request=response_text
            )
    except Exception as e:
        print(f"Error during streaming interaction: {str(e)}...")
        return ApplicationEvent(
            type=ApplicationEventType.ERROR,
            request=str(e)[:100]
        )

# Step 5: Trigger Synthesize and Play Events

class EventHandler(AssistantEventHandler):
    def __init__(self, streaming_manager):
        super().__init__()  # Ensure the base class is properly initialized
        self.streaming_manager = streaming_manager

    @override
    def on_text_created(self, text) -> None:
        print(f"\nassistant > ", end="", flush=True)
        self.streaming_manager.response_text += text.value  # Access the string value

    @override
    def on_text_delta(self, delta, snapshot):
        print(delta.value, end="", flush=True)
        self.streaming_manager.response_text += delta.value  # Access the string value

    @override
    def on_tool_call_created(self, tool_call):
        print(f"\nassistant > {tool_call.type}\n", flush=True)

    @override
    def on_tool_call_delta(self, delta, snapshot):
        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="", flush=True)
                self.streaming_manager.response_text += delta.code_interpreter.input  # Append the input to the response text
            if delta.code_interpreter.outputs:
                print(f"\n\noutput >", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        print(f"\n{output.logs}", flush=True)
                        self.streaming_manager.response_text += output.logs  # Append the logs to the response text

    @override
    def on_event(self, event):
        if event.event == 'thread.run.requires_action':
            run_id = event.data.id
            self.handle_requires_action(event.data, run_id)

    def handle_requires_action(self, data, run_id):
        tool_outputs = []

        for tool in data.required_action.submit_tool_outputs.tool_calls:
            if tool.function.name == "send_text_message":
                arguments = tool.function.parameters
                if isinstance(arguments, str):
                    arguments = json.loads(arguments)
                message = arguments.get("message", "")
                send_result = send_text_message({"message": message})
                tool_outputs.append({"tool_call_id": tool.id, "output": send_result})

        self.submit_tool_outputs(tool_outputs, run_id)

    def submit_tool_outputs(self, tool_outputs, run_id):
        with self.streaming_manager.openai_client.beta.threads.runs.submit_tool_outputs_stream(
            thread_id=self.streaming_manager.thread_manager.thread_id,
            run_id=run_id,
            tool_outputs=tool_outputs,
            event_handler=self,
        ) as stream:
            for text in stream.text_deltas:
                print(text, end="", flush=True)
            print()

def send_text_message(parameters):
    # Implement the function to send a text message via Zapier
    # Return "Success!" or an appropriate error message
    return "Success!"